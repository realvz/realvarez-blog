<!DOCTYPE html><html domain=realvarez.com ga-id=G-B5XRB3YVTR lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><link href="/img/favicon/favicon-192x192.png?hash=ae2c3e88f6" rel=icon type=image/png><meta content=#f9c412 name=theme-color><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name=robots><title>Autoscale Kubernetes Metrics Server</title><meta content="Autoscale Kubernetes Metrics Server" property=og:title><meta content="This is a post on My Blog about touchpoints and circling wagons." name=description><meta content="This is a post on My Blog about touchpoints and circling wagons." property=og:description><meta content=summary_large_image name=twitter:card><meta content=@realz name=twitter:site><meta content=@realz name=twitter:creator><link href=https://blog.realvarez.com/posts/autoscale-metrics-server/ rel=canonical><meta content=no-referrer-when-downgrade name=referrer><link href=/feed/feed.xml rel=alternate type=application/atom+xml title="Re Alvarez Parmar's Blog"><link href=/ rel=preconnect crossorigin=""><link href=/fonts/Inter-3.19.var.woff2 rel=preload type=font/woff2 as=font crossorigin=""><script async defer src="/js/min.js?hash=8db32dcb3a"></script><script async defer src="/js/cached.js?hash=29681a444d"></script><script csp-hash="">if (/Mac OS X/.test(navigator.userAgent))document.documentElement.classList.add('apple')</script><style>:root{--primary: #f9c412;--primary-dark: #e7bf60;--main-width: calc(100vw - 3em)}main img{content-visibility:auto}article *{scroll-margin-top:50px}header nav{position:fixed;padding:.375em 1.5em;background:rgba(255,255,255,.9);font-weight:200;text-align:right}@media (min-width:37.5em){:root{--main-width: calc(37.5em - 3em)}}dialog,share-widget{position:fixed;opacity:.9}share-widget{right:20px;bottom:20px}share-widget div{width:30px;height:30px;background-image:url(/img/share.svg);background-repeat:no-repeat;background-position:center}.apple share-widget div{background-image:url(/img/share-apple.svg)}body,share-widget button{margin:0}share-widget button:active{transform:scale(1.2)}dialog{background-color:#8dff80;z-index:1000}header aside{font-style:italic}#nav{z-index:2;position:relative}#reading-progress,header nav{z-index:1;width:100vw;left:0;top:0}#reading-progress{background-color:var(--primary);position:absolute;bottom:0;transform:translate(-100vw,0);will-change:transform;pointer-events:none}@font-face{font-display:optional;font-family:"Inter UI";font-weight:100 900;font-style:oblique 0deg 10deg;src:url(/fonts/Inter-3.19.var.woff2) format("woff2")}button,html{line-height:1.15}html{-webkit-text-size-adjust:100%;font-family:Inter UI,sans-serif;--font-family: Inter UI, sans-serif}h1{font-size:3em;line-height:1.25;margin:.67em 0 .5em;font-size:2.074rem}a{background-color:transparent;color:#f9c412;text-decoration:none;color:var(--primary)}strong{font-weight:700}img{border-style:none;max-width:100%;height:auto;margin:0 auto}button{font-family:inherit;font-size:100%;overflow:visible;text-transform:none}[type=button],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}template{display:none}h2{font-size:2.5em;line-height:1.2;margin-bottom:.6em}h1,h2,h4{margin-bottom:1.36rem}h4{font-size:1.5em;margin-bottom:1em;line-height:1.5em;line-height:1.6rem;font-size:1.2rem}body,p,pre,ul{font-size:1em}p,pre,ul{margin-bottom:1.5em}h1,h2{line-height:2.4rem}h2{font-size:1.728rem}body,p,pre,ul{font-size:1rem;line-height:1.6}p,pre,ul{margin-bottom:1.36rem}@media (min-width:600px){h1{font-size:4.3978rem;line-height:4.4rem}h2{font-size:3.1097rem;line-height:3.52rem}h4{font-size:1.5554rem;line-height:1.76rem}body,p,pre,ul{font-size:1.1rem;line-height:1.6}h1,h2,h4,p,pre,ul{margin-bottom:1.496rem}}@media (min-width:1200px){h1{font-size:6.0756rem;line-height:6.72rem}h2{font-size:4.05rem;line-height:4.8rem}h4{font-size:1.8rem;line-height:1.92rem}body,p,pre,ul{font-size:1.2rem;line-height:1.6}h1,h2,h4,p,pre,ul{margin-bottom:1.632rem}}code,pre{overflow-x:auto}pre{font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace}pre:has(code:not([class])){background:#2d2d2d}pre code:not([class]){color:#ccc;padding:0;overflow-x:scroll}button,code{border-radius:.3em}code{color:#e2777a;padding:0 .3em;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:90%;background:#2d2d2d}h1,h2,h4{font-family:var(--font-family)}h2,h4{font-style:italic}a:hover{text-decoration:underline}dt{font-weight:600;padding-top:.75em;padding-left:.75em}button{max-width:100%;background:#f2f2f2;color:#191919;cursor:pointer;display:inline-block;padding:.75em 1.5em;text-align:center;margin:0 .75em 1.5em 0}button:hover{background:#d9d9d9;color:#000}button:not([disabled]){background:#f9c412;color:#181818;background:var(--primary)}button:not([disabled]):hover{background:#ba9005;color:#000;background:var(--primary-dark)}*{border:0;box-sizing:border-box}body,header nav a{font-family:var(--font-family)}body{background:#181818;color:#eee}header{padding:4.5em 1.5em 3em;width:37.5em;margin:0 auto;text-align:center;max-width:100%;display:flex;align-items:center;flex-direction:column}header p,ul{margin-top:0}header nav h1{float:left;font-size:inherit;line-height:inherit;margin:0;text-align:left}header nav a{font-weight:700;text-decoration:none;color:#181818;margin-left:1.5em}header nav a:first-of-type{margin-left:auto}header nav a:last-of-type{margin-right:1.5em}main{max-width:70rem;margin:0 auto;border-top:.5px solid #ccc}footer{background:rgba(0,0,0,.8);color:#fff;padding:3em;text-align:center}footer>*{margin:1.5em}footer nav a img{vertical-align:middle}footer nav,footer p{font-size:90%}article{max-width:100%;padding:1.5em;width:37.5em;margin:0 auto}</style><header><nav><div id=nav><h1><a href=/ title=Homepage>Re Alvarez Parmar's Blog</a></h1><a href=/about/ >About</a></div><div id=reading-progress aria-hidden=true></div></nav><aside>8 min read.</aside><dialog id=message></dialog><noscript><img alt="" height=1 src="/api/ga?v=1&_v=j83&t=pageview&dr=https%3A%2F%2Fno-script.com&_s=1&dh=realvarez.com&dp=%2Fposts%2Fautoscale-metrics-server%2F&ul=en-us&de=UTF-8&dt=Autoscale%20Kubernetes%20Metrics%20Server&tid=G-B5XRB3YVTR" style=display:none width=1></noscript></header><main><article><p>Many organizations are happy to standardize their infrastructure platform on Kubernetes. Kubernetes gives engineers a consistent platform across cloud providers and on premises. It abstracts underlying infrastructure so engineers can focus on writing code without having tight-coupling with methods for load balancing, observability, configuration, secrets management, etc.<p>I frequently speak with organizations that run their entire workloads in one or two Kubernetes clusters. Effectively, they have moved their entire data centers into Kubernetes clusters.<p>Now, Kubernetes is not without its flaws. Especially when operating at scale. Once clusters go beyond hundreds of nodes, nuanced behavior starts showing up. Besides scaling the Kubernetes control plane and data plane, platform teams have to scale Kubernetes components like CoreDNS, core components, and add-ons.<p>In this post, I am going to show how to scale the metrics server add-on, so when your cluster scales the Horizontal Pod Autoscaler can reliably scale your workload.<h2 id=scaling-kubernetes-add-ons>Scaling Kubernetes add-ons <a href=#scaling-kubernetes-add-ons class=direct-link>#</a></h2><p>Kubernetes add-ons are software packages that extend the functionality of Kubernetes. Vanilla Kubernetes clusters lack capabilities that most production clusters require. For example, data plane scaling functionality in Kubernetes is provided by the <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>Kubernetes Cluster Autoscaler</a>. Metrics server collects Node and Pod metrics. Log aggregators like Fluent Bit and Fluentd allow you to collect Kubernetes application and system logs.<p>It is a best practice to deploy these add-ons with resource limits to account for bugs and memory leaks. Requests and limits allow us to control system resource allocated to each Pod. This feature makes it safer to run multiple Pods on a node without worrying about resource contention or oversubscription.<p>Add-ons are deployed either as DaemonSets or Deployments. As a cluster scales, DaemonSets scale automatically as they run once per node. However, add-ons deployed as Deployments do not scale automatically because they are unaware of the size of the clusterâ€™s scale.<p>As the cluster scales, add-ons such as <a href=https://github.com/kubernetes-sigs/metrics-server>metrics-server</a> and <a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a> have to hold more data in memory. The default resource requests on the metrics-server are sized for clusters of up to 100 nodes. In clusters larger than that, the metrics-server can run out of memory, which breaks the Horizontal Pod Autoscaler.<p>As a result, operators have to scale add-ons, such as the metrics server, vertically as the cluster scales. <a href=https://github.com/kubernetes/autoscaler/tree/master/addon-resizer>Addon-resizer</a> is an open source tool you can use to scale Deployments in proportion to the data plane. While the Kubernetes <a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>Cluster Proportional Autoscaler</a> scales Deployments horizontally, the addon-resizer scales Deployments vertically.<p>Some cloud providers use addon-resizer to scale the metrics-server add-on. Amazon EKS currently doesn't automatically scale metrics-server. I am going to run addon-resizer to autoscale the metrics-server Deployment in an EKS cluster.<h2 id=addon-resizer>Addon-resizer <a href=#addon-resizer class=direct-link>#</a></h2><p>Addon-resizer is a container that vertically scales a Deployment based on the number of nodes in your cluster. It scales Deployments linearly as the clusterâ€™s data plane grows and shrinks.<p>The container monitors your cluster periodically and increases or decreases the requests and limits of a Deployment in proportion to the number of nodes. Vertical scaling implies that <strong>addon-resizer will recreate the Pods</strong> with newer resource limits.<p>At the core of addon-resizer lies the *nanny *program.<pre><code>Usage of ./pod_nanny:
      --acceptance-offset=20: A number from range 0-100. The dependent's resources are rewritten when they deviate from expected by a percentage that is higher than this threshold. Can't be lower than recommendation-offset.
      --alsologtostderr[=false]: log to standard error as well as files
      --container="pod-nanny": The name of the container to watch. This defaults to the nanny itself.
      --cpu="MISSING": The base CPU resource requirement.
      --deployment="": The name of the deployment being monitored. This is required.
      --extra-cpu="0": The amount of CPU to add per node.
      --extra-memory="0Mi": The amount of memory to add per node.
      --extra-storage="0Gi": The amount of storage to add per node.
      --log-flush-frequency=5s: Maximum number of seconds between log flushes
      --log_backtrace_at=:0: when logging hits line file:N, emit a stack trace
      --log_dir="": If non-empty, write log files in this directory
      --logtostderr[=true]: log to standard error instead of files
      --memory="MISSING": The base memory resource requirement.
      --namespace="": The namespace of the ward. This defaults to the nanny pod's own namespace.
      --pod="": The name of the pod to watch. This defaults to the nanny's own pod.
      --poll-period=10000: The time, in milliseconds, to poll the dependent container.
      --recommendation-offset=10: A number from range 0-100. When the dependent's resources are rewritten, they are set to the closer end of the range defined by this percentage threshold.
      --stderrthreshold=2: logs at or above this threshold go to stderr
      --storage="MISSING": The base storage resource requirement.
      --v=0: log level for V logs
      --vmodule=: comma-separated list of pattern=N settings for file-filtered logging
</code></pre><p>The nanny program takes the base CPU and memory and adds extra resources per node. Hereâ€™s the formula it uses:<pre><code>Base  CPU + (Extra CPU * Nodes)
</code></pre><p>Letâ€™s say we allocate 100m CPU and 200Mi memory to a container in our cluster. We configure addon-resizer to add 1m CPU and 2Mi memory per node. When the cluster scales to 75 nodes, addon-resizer will scale the target container using the formula below:<pre><code>100m+(1m*75) = 175m
</code></pre><p>It will also increase memory:<pre><code>200Mi + (2Mi*75)= 350Mi
</code></pre><h2 id=scale-metrics-server>Scale metrics-server <a href=#scale-metrics-server class=direct-link>#</a></h2><p>The first question you may have is when should you scale metrics-server. The default resource configuration in metrics-server Deployment is recommended for clusters of up to 100 nodes. Beyond that you may notice the metrics-server restarting frequently (as it gets killed by Kubernetes Out of Memory killer).<p>When metrics-server needs more resources than allocated <code>kubectl top nodes</code> and <code>kubectl top pods</code> will fail. You may get the following error message:<p><code>unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)</code><p>Also, the Horizontal Pod Autoscaler may stop working. If you run <code>kubectl get apiservices v1beta1.metrics.k8s.io</code>, you may get the following message:<pre><code>NAME                     SERVICE                      AVAILABLE                      AGE
v1beta1.metrics.k8s.io   kube-system/metrics-server   False (FailedDiscoveryCheck)   12m
</code></pre><h2 id=deploy-addon-resizer>Deploy addon-resizer <a href=#deploy-addon-resizer class=direct-link>#</a></h2><p>The addon-resizer container can run in its own Pod or as a sidecar. Weâ€™re going to deploy the container as a sidecar in the metrics-server Deployment.<p>The metrics server defaults to 100m CPU and 200Mi memory. Get the current limits:<pre><code>kubectl -n kube-system get \
  deployments metrics-server \
  -o jsonpath='{.spec.template.spec.containers[].resources}'
</code></pre><p>Hereâ€™s the output from my cluster:<pre><code>{"requests":{"cpu":"100m","memory":"200Mi"}}
</code></pre><p>My cluster currently has 5 nodes. Iâ€™ll configure the addon-resizer to scale the metrics-server Deployment vertically by adding 1m CPU per node in addition to the base CPU which is set to 20m. The base memory is 15Mi, and addon-resizer will increase metrics-server memory by 2Mi per node. I took these values from addon-resizer recommendations.<p>Deploy the manifest to create a ClusterRole, Role, and ClusterRoleBinding that gives the metrics-server service account the permissions to patch the metrics-server Deployment:<pre><code>cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: eks:metrics-server-nanny
  labels:
    k8s-app: metrics-server
rules:
- nonResourceURLs:
  - /metrics
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: eks:metrics-server-nanny
  labels:
    k8s-app: metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: eks:metrics-server-nanny
subjects:
  - kind: ServiceAccount
    name: metrics-server
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: metrics-server-nanny
  namespace: kube-system
  labels:
    k8s-app: metrics-server
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - apps
  resources:
  - deployments
  resourceNames:
  - metrics-server
  verbs:
  - get
  - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metrics-server-nanny
  namespace: kube-system
  labels:
    k8s-app: metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: metrics-server-nanny
subjects:
  - kind: ServiceAccount
    name: metrics-server
    namespace: kube-system
EOF
</code></pre><p>Create a patch file to add the nanny container to the metrics-server Deployment.<pre><code>cat > metrics-server-addon-patch.yaml &lt;&lt; EOF
spec:
  template:
    spec:
      containers:
      - name: metrics-server-nanny
          image: registry.k8s.io/autoscaling/addon-resizer:1.8.14
          resources:
            limits:
              cpu: 40m
              memory: 25Mi
            requests:
              cpu: 40m
              memory: 25Mi
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          command:
            - /pod_nanny
            - --cpu=20m
            - --extra-cpu=1m
            - --memory=15Mi
            - --extra-memory=2Mi
            - --threshold=5
            - --deployment=metrics-server
            - --container=metrics-server
            - --poll-period=30000
            - --estimator=exponential
            - --minClusterSize=10
            - --use-metrics=true
EOF

kubectl -n kube-system patch deployments metrics-server --patch-file metrics-server-addon-patch.yaml
</code></pre><p>If you install the metrics-server as documented in Amazon EKS documentation, it requests 100m CPU and 200Mi memory. After deploying the patch above, the metrics server requests set to 40m CPU and 15Mi memory. As you add more nodes, the nanny will automatically adjust the requests and limits for the metrics-server container.<p>I scaled my cluster to 15 nodes and the addon-resizer configured metrics-server requests to 35m CPU and 45Mi memory.<pre><code>20baseCPU+(15nodes*1extraCPU) = 35m

</code></pre><p>Memory calculation<pre><code>15baseMemory+(15nodes*2extraMemory) = 45Mi

</code></pre><p>Addon-resizer calculates the resources reservation for the metrics-server container and restarts the container automatically.<h2 id=what-about-scaling-metrics-server-horizontally%3F>What about scaling metrics server horizontally? <a href=#what-about-scaling-metrics-server-horizontally%3F class=direct-link>#</a></h2><p>While you can run metrics server in high-availability mode, its main purpose is ensuring that if one of the metrics server Pods terminate, the other one can still serve requests.<p>Running two instances of metrics server doesnâ€™t provide any further benefits. Both instances will scrape all nodes to collect metrics, but only one instance will be actively serving metrics API.<h2 id=conclusion>Conclusion <a href=#conclusion class=direct-link>#</a></h2><p>The Amazon EKS documentation currently documents steps to deploy metrics-server in static configuration. You can use addon-resizer to autoscale the metrics-server based on the number of nodes in your cluster.</p><share-widget><button aria-label=Share href=https://blog.realvarez.com/posts/autoscale-metrics-server/ on-click=share><div></div></button></share-widget><script type=application/ld+json>{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Autoscale Kubernetes Metrics Server",
  "image": [],
  "author": "", 
  "genre": "Insert a schema.org genre", 
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "/img/favicon/favicon-192x192.png?hash=ae2c3e88f6"
    }
  },
  "url": "https://blog.realvarez.com/posts/autoscale-metrics-server/",
  "mainEntityOfPage": "https://blog.realvarez.com/posts/autoscale-metrics-server/",
  "datePublished": "2022-11-03",
  "dateModified": "2023-04-09",
  "description": "Many organizations are happy to standardize their infrastructure platform on Kubernetes. Kubernetes gives engineers a consistent platform..."
}</script><p>Published <time datetime=2022-11-03>03 Nov 2022</time></article></main><footer><a href=/about/ ></a></footer>